<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0" name="viewport">

    <title>6th ABAW Workshop & Competition</title>
    <meta content="" name="description">
    <meta content="" name="keywords">

    <!-- Favicons -->
    <link href="img/favicon.png" rel="icon">
    <link href="img/apple-touch-icon.png" rel="apple-touch-icon">

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i"
          rel="stylesheet">

    <!-- Vendor CSS Files -->
    <link href="vendor/aos/aos.css" rel="stylesheet">
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
    <link href="vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
    <link href="vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
    <link href="vendor/remixicon/remixicon.css" rel="stylesheet">
    <link href="vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

    <!-- Template Main CSS File -->
    <link href="css/style.css" rel="stylesheet">

    <!-- =======================================================
    * Template Name: Gp
    * Updated: Nov 25 2023 with Bootstrap v5.3.2
    * Template URL: https://bootstrapmade.com/gp-free-multipurpose-html-bootstrap-template/
    * Author: BootstrapMade.com
    * License: https://bootstrapmade.com/license/
    ======================================================== -->
</head>

<body>

<!-- ======= Header ======= -->
<header id="header" class="fixed-top ">
    <div class="container d-flex align-items-center justify-content-lg-between">

        <h1 class="logo me-auto me-lg-0"><a href="index.html">6th ABAW</a></h1>
        <!-- Uncomment below if you prefer to use an image logo -->
        <!-- <a href="index.html" class="logo me-auto me-lg-0"><img src="img/logo.png" alt="" class="img-fluid"></a>-->

        <nav id="navbar" class="navbar order-last order-lg-0">
            <ul>
                <li><a class="nav-link scrollto active" href="#hero">Home</a></li>
                <li><a class="nav-link scrollto" href="#about">About</a></li>
                <li><a class="nav-link scrollto" href="#team">Organisers</a></li>

                <li class="dropdown"><a href="#portfolio"><span>Workshop</span> <i class="bi bi-chevron-down"></i></a>
                    <ul>
                        <li><a href="#portfolio2">Agenda</a></li>
                        <li><a href="#portfolio3">Keynote Speaker</a></li>
                    </ul>
                </li>

                <li class="dropdown"><a href="#clients"><span>Competition</span> <i class="bi bi-chevron-down"></i></a>
                    <ul>
                        <li><a href="#features">VA Estimation</a></li>
                        <li><a href="#services">Expr Recognition</a></li>
                        <li><a href="#cta">AU Detection</a></li>
                        <li><a href="#counts">CE Recognition</a></li>
                        <li><a href="#counts2">EMI Estimation</a></li>
                        <li><a href="#counts3">Leaderboards</a></li>
                    </ul>
                </li>

            </ul>
        </nav><!-- .navbar -->

        <a href="https://cmt3.research.microsoft.com/ABAW2024" class="get-started-btn scrollto">Submission Site</a>

    </div>
</header><!-- End Header -->

<!-- ======= Hero Section ======= -->
<section id="hero" class="d-flex align-items-center justify-content-center">
    <div class="container" data-aos="fade-up">

        <!-- <div class="row justify-content-center" data-aos="fade-up" data-aos-delay="150">   -->
        <!--   <div class="col-xl-6 col-lg-8">   -->
        <h1>6th Workshop and Competition on </h1>
        <br>
        <h1> Affective Behavior Analysis in-the-wild (ABAW)</h1>
        <h2>in conjunction with the IEEE Computer Vision and Pattern Recognition Conference (CVPR), 2024 </h2>
        <h2> 13:30 - 18:00 PDT, 18 June 2024, Arch 212, Seattle Convention Center, Seattle WA, USA </h2>
    </div>
    </div>


    </div>
</section><!-- End Hero -->


<!-- ================================================================================================== -->
<!-- ================================================================================================== -->
<!-- ================================================================================================== -->


<main id="main">

    <!-- ======= About Section ======= -->
    <section id="about" class="about">
        <div class="container" data-aos="fade-up">

            <div class="row">
                <div class="col-lg-6 pt-4 pt-lg-0 order-2 order-lg-1 content" data-aos="fade-left" data-aos-delay="100">
                    <img src="img/about.png" class="img-fluid" width="300" height="300" alt="">
                </div>
                <div class="col-lg-6 order-1 order-lg-2" data-aos="fade-right" data-aos-delay="100">
                    <h3>About ABAW</h3>
                    <p>
                        The ABAW Workshop and Competition has a unique aspect of fostering cross-pollination of
                        different disciplines, bringing together experts (from academia, industry, and government) and
                        researchers of mobile and ubiquitous computing, computer vision and pattern recognition,
                        artificial intelligence and machine learning, multimedia, robotics, HCI, ambient intelligence
                        and psychology. The diversity of human behavior, the richness of multi-modal data that arises
                        from its analysis, and the multitude of applications that demand rapid progress in this area
                        ensure that our events provide a timely and relevant discussion and dissemination platform.
                    </p>
                    <p>
                        The ABAW Workshop and Competition is a continuation of the respective Workshops and Competitions
                        held at
                        <a href="https://ibug.doc.ic.ac.uk/resources/cvpr-2023-5th-abaw/">IEEE CVPR 2023</a>,
                        <a href="https://ibug.doc.ic.ac.uk/resources/eccv-2023-4th-abaw/">ECCV 2022</a>,
                        <a href="https://ibug.doc.ic.ac.uk/resources/cvpr-2022-3rd-abaw/">IEEE CVPR 2022</a>,
                        <a href="https://ibug.doc.ic.ac.uk/resources/iccv-2021-2nd-abaw/">ICCV 2021</a>,
                        <a href="https://ibug.doc.ic.ac.uk/resources/fg-2020-competition-affective-behavior-analysis/">IEEE
                            FG 2020 (a)</a>,
                        <a href="https://ibug.doc.ic.ac.uk/resources/affect-recognition-wild-unimulti-modal-analysis-va/">IEEE
                            FG 2020 (B)</a> and
                        <a href="https://ibug.doc.ic.ac.uk/resources/first-affect-wild-challenge/">IEEE CVPR 2017</a>
                        Conferences.
                    </p>
                </div>
            </div>

        </div>
    </section><!-- End About Section -->


    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->

    <!-- ======= Team Section ======= -->
    <section id="team" class="team">
        <div class="container" data-aos="fade-up">

            <div class="d-flex align-items-center justify-content-center" data-aos="fade-right" data-aos-delay="100">
                <h2>Organisers</h2>

            </div>

            <div class="row">

                <div class="col-lg col-md d-flex align-items-stretch">
                    <div class="member" data-aos="fade-up" data-aos-delay="100">
                        <div class="member-img">
                            <img src="img/jim.jpg" class="img-fluid" width="150" height="150" alt="">
                            <div class="social">
                                <a href="https://sites.google.com/view/dimitrioskollias/home?authuser=0&pli=1"><i
                                        class="bi bi-list"></i></a>
                                <a href="https://scholar.google.com/citations?user=360Gmc0AAAAJ&hl=en"><i
                                        class="bi-journal-richtext"></i></a>
                            </div>
                        </div>
                        <div class="member-info">
                            <h4>Dimitrios Kollias </h4>
                            <span>Queen Mary University </span> <span> of London, UK </span>
                            <span> d.kollias@qmul.ac.uk</span>
                        </div>
                    </div>
                </div>

                <div class="col-lg col-md d-flex align-items-stretch">
                    <div class="member" data-aos="fade-up" data-aos-delay="100">
                        <div class="member-img">
                            <img src="img/stefanos.jpg" class="img-fluid" width="150" height="150" alt="">
                            <div class="social">
                                <a href="https://www.imperial.ac.uk/people/s.zafeiriou"><i class="bi bi-list"></i></a>
                                <a href="https://scholar.google.com/citations?user=QKOH5iYAAAAJ&hl=en&oi=ao"><i
                                        class="bi-journal-richtext"></i></a>
                            </div>
                        </div>
                        <div class="member-info">
                            <h4>Stefanos Zafeiriou </h4>
                            <span>Imperial College London, UK </span> <span>  s.zafeiriou@imperial.ac.uk </span>
                        </div>
                    </div>
                </div>


                <div class="col-lg col-md d-flex align-items-stretch">
                    <div class="member" data-aos="fade-up" data-aos-delay="100">
                        <div class="member-img">
                            <img src="img/kotsia.jpg" class="img-fluid" width="150" height="150" alt="">
                            <div class="social">
                                <a href="https://scholar.google.co.uk/citations?user=RwDV2kAAAAAJ&hl=en"></i><i
                                        class="bi-journal-richtext"></i></a>
                                <a href="https://uk.linkedin.com/in/irene-kotsia-77690b5" class="linkedin"><i
                                        class="bx bxl-linkedin"></i></a>
                            </div>
                        </div>
                        <div class="member-info">
                            <h4>Irene Kotsia </h4>
                            <span>Cogitat Ltd, UK</span>
                            <span>   irene@cogitat.io   </span>
                        </div>
                    </div>
                </div>


                <div class="col-lg col-md d-flex align-items-stretch">
                    <div class="member" data-aos="fade-up" data-aos-delay="100">
                        <div class="member-img">
                            <img src="img/panos.png" class="img-fluid" width="150" height="150" alt="">
                            <div class="social">
                                <a href="https://scholar.google.co.uk/citations?user=rWKaCDAAAAAJ&hl=en"><i
                                        class="bi-journal-richtext"></i></a>
                                <a href="https://www.linkedin.com/in/panagiotis-tzirakis-46a21639" class="linkedin"><i
                                        class="bx bxl-linkedin"></i></a>
                            </div>
                        </div>
                        <div class="member-info">
                            <h4>Panagiotis Tzirakis </h4>
                            <span>  Hume AI, USA     </span>
                            <span>   panagiotis@hume.ai   </span>
                        </div>
                    </div>
                </div>


                <div class="col-lg col-md d-flex align-items-stretch">
                    <div class="member" data-aos="fade-up" data-aos-delay="100">
                        <div class="member-img">
                            <img src="img/alan.png" class="img-fluid" width="150" height="150" alt="">
                            <div class="social">
                                <a href="https://scholar.google.com/citations?user=-i9gbsAAAAAJ&hl=en"><i
                                        class="bi-journal-richtext"></i></a>
                                <a href="https://www.linkedin.com/in/alan-cowen" class="linkedin"><i
                                        class="bx bxl-linkedin"></i></a>
                            </div>
                        </div>
                        <div class="member-info">
                            <h4>Alan Cowen </h4>
                            <span>  Hume AI, USA     </span>
                            <span>   alan@hume.ai   </span>
                        </div>
                    </div>
                </div>

            </div>

            <p></p>
            <div class="col-lg-6 order-1 order-lg-2" data-aos="fade-right" data-aos-delay="100">
                <h3>Data Chairs</h3>

            </div>

            <p>
            <h5> Alice Baird, &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp Hume AI, USA </h5>
            <h5> Chris Gagne, &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp Hume AI, USA </h5>
            <h5>Chunchang Shao, &nbsp &nbsp Queen Mary University of London, UK </h5>
            <h5>Guanyu Hu, &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp Queen Mary University of London, UK & Xi'an
                Jiaotong University, China </h5>
            </p>


        </div>

    </section><!-- End Team Section -->


    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->


    <!-- ======= Portfolio Section ======= -->
    <section id="portfolio" class="portfolio">
        <div class="container" data-aos="fade-up">


            <div class="d-flex align-items-center justify-content-center">
                <h1>The Workshop</h1>
            </div>

            <br> </br>

            <div class="section-title">
                <h2>Call for Papers</h2>

            </div>

            <div class="text">

                <p>
                    This Workshop will solicit contributions on the recent progress of recognition, analysis,
                    generation-synthesis and modelling of face, body, gesture, speech, audio, text and language while
                    embracing the most advanced systems available for such in-the-wild (i.e., in unconstrained
                    environments) analysis, and across modalities like face to voice. In parallel, this Workshop will
                    solicit contributions towards building fair, explainable, trustworthy and privacy-aware models that
                    perform well on all subgroups and improve in-the-wild generalisation. </p>


                <p>
                    Original high-quality contributions, in terms of databases, surveys, studies, foundation models,
                    techniques and methodologies (either uni-modal or multi-modal; uni-task or multi-task ones) are
                    solicited on -but are not limited to- the following topics: </p>

                <ul>
                    <p><i class="ri-check-double-line"></i> facial expression (basic, compound or other) or
                        micro-expression analysis</p>
                    <p><i class="ri-check-double-line"></i> facial action unit detection</p>
                    <p><i class="ri-check-double-line"></i> valence-arousal estimation</p>
                    <p><i class="ri-check-double-line"></i> physiological-based (e.g.,EEG, EDA) affect analysis</p>
                    <p><i class="ri-check-double-line"></i> face recognition, detection or tracking</p>
                    <p><i class="ri-check-double-line"></i> body recognition, detection or tracking</p>
                    <p><i class="ri-check-double-line"></i> gesture recognition or detection </p>
                    <p><i class="ri-check-double-line"></i> pose estimation or tracking </p>
                    <p><i class="ri-check-double-line"></i> activity recognition or tracking </p>
                    <p><i class="ri-check-double-line"></i> lip reading and voice understanding </p>
                    <p><i class="ri-check-double-line"></i> face and body characterization (e.g., behavioral
                        understanding) </p>

                    <p><i class="ri-check-double-line"></i> characteristic analysis (e.g., gait, age, gender, ethnicity
                        recognition) </p>

                    <p><i class="ri-check-double-line"></i> group understanding via social cues (e.g., kinship,
                        non-blood relationships, personality) </p>
                    <p><i class="ri-check-double-line"></i> video, action and event understanding </p>
                    <p><i class="ri-check-double-line"></i> digital human modeling
                    </p>
                    <p><i class="ri-check-double-line"></i> characteristic analysis (e.g., gait, age, gender, ethnicity
                        recognition) </p>
                    <p><i class="ri-check-double-line"></i> violence detection </p>
                    <p><i class="ri-check-double-line"></i> autonomous driving </p>
                    <p><i class="ri-check-double-line"></i> domain adaptation, domain generalisation, few- or zero-shot
                        learning for the above cases </p>
                    <p><i class="ri-check-double-line"></i> fairness, explainability, interpretability, trustworthiness,
                        privacy-awareness, bias mitigation and/or subgroup distribution shift analysis for the above
                        cases </p>
                    <p><i class="ri-check-double-line"></i> editing, manipulation, image-to-image translation, style
                        mixing, interpolation, inversion and semantic diffusion for all afore mentioned cases </p>

                </ul>


                <br> </br>


                <div class="section-title">
                    <h2>Workshop Important Dates</h2>
                </div>


                <br> <span> <em>Paper Submission Deadline: </em>  &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp 23:59:59 AoE (Anywhere on Earth) March 30, 2024  </span> </br>
                <br> <span>  <em>Review decisions sent to authors; Notification of acceptance: </em>  &nbsp &nbsp &nbsp  April 10, 2024 </span> </br>
                <br> <span>   <em>Camera ready version: </em> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp April  14, 2024  </span> </br>

                <br></br>
                <br></br>
                <div class="section-title">
                    <h2>Submission Information</h2>

                </div>


                <p>The paper format should adhere to the paper submission guidelines for main CVPR 2024 proceedings
                    style. Please have a look at the Submission Guidelines Section <a
                            href="https://cvpr2023.thecvf.com/Conferences/2024/AuthorGuidelines">here</a>. </p>

                <p>We welcome full long paper submissions (between 5 and 8 pages, excluding references or supplementary
                    materials; a paper submission should be at least 4 pages long to be considered for publication). All
                    submissions must be anonymous and conform to the CVPR 2024 standards for double-blind review. </p>

                <p>All papers should be submitted using <a href="https://cmt3.research.microsoft.com/ABAW2024">this CMT
                    website</a>. </p>

                <p>All accepted manuscripts will be part of CVPR 2024 conference proceedings. </p>
                At the day of the workshop, oral presentations will be conducted by authors who are attending in-person.


            </div>
        </div>


    </section><!-- End Portfolio Section -->


    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->


    <!-- ======= Portfolio Section ======= -->
    <section id="portfolio2" class="portfolio2">
        <div class="container" data-aos="fade-up">


            <div class="d-flex align-items-center justify-content-center">
                <h1>The Workshop's Agenda</h1>
            </div>

            <br> </br>

            <img src="img/6th ABAW Agenda2.png" alt="Agenda" class="center" width="898" height="577"/>

        </div>
        </div>


    </section><!-- End Portfolio Section -->


    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->


    <!-- ======= Portfolio Section ======= -->
    <section id="portfolio3" class="portfolio2">
        <div class="container" data-aos="fade-up">


            <div class="d-flex align-items-center justify-content-center">
                <h1>Keynote Speaker: Angelica Lim</h1>
            </div>

            <br> </br>

            <h3>Biography</h3>
            <p>
                Dr. Angelica Lim is the Director of the <a href="https://www.rosielab.ca">Rosie Lab</a>, and an
                Assistant Professor in the School of Computing Science at Simon Fraser University (SFU).
                Previously, she led the Emotion and Expressivity teams for the Pepper humanoid robot at SoftBank
                Robotics.
                She received her B.Sc. in Computing Science with Artificial Intelligence Specialization from SFU and a
                Ph.D. and M.Sc. in Computer Science (Intelligence Science) from Kyoto University, Japan.
                She and her team have received Best Paper in Entertainment Robotics and Cognitive Robotics Awards at
                IROS 2011 and 2022, and Best Demo and LBR at HRI 2021 and 2023.
                She has been featured on the BBC, TEDx, hosted a TV documentary on robotics, and was recently featured
                in Forbes 20 Leading Women in AI.
                Her research interests include multimodal machine learning, affective computing, and human-robot
                interaction.
            </p>
            <br> </br>

            <h3>Title: Social Signals in the Wild: Multimodal Machine Learning for Human-Robot Interaction</h3>


            <p> Science fiction has long promised us interfaces and robots that interact with us as smoothly as humans
                do - Rosie the Robot from The Jetsons, C-3PO from Star Wars, and Samantha from Her.
                Today, interactive robots and voice user interfaces are moving us closer to effortless, human-like
                interactions in the real world.
                In this talk, I will discuss the opportunities and challenges in creating technologies that can finely
                analyze, detect and generate non-verbal communication in context, including gestures, gaze, auditory
                signals, and facial expressions.
                Specifically, I will discuss how we might allow robots to understand human social signals (including
                emotions, mental states, and attitudes) across cultures as well as recognize and generate expressions
                with controllability and diversity in mind.
            </p>


        </div>
        </div>


    </section><!-- End Portfolio Section -->


    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->


    <!-- ======= Clients Section ======= -->
    <section id="clients" class="clients">
        <div class="container" data-aos="zoom-in">


            <div class="d-flex align-items-center justify-content-center">
                <h1>The Competition</h1>
            </div>


            <br></br>

            <p> The Competition is a continuation of the ABAW Competition held last year in CVPR, the year before in
                ECCV and CVPR, the year before in ICCV and the year before in IEEE FG. It is split into the five below
                mentioned Challenges. Participants are invited to participate in at least one of these Challenges.
            </p>


            <br></br>


            <h2>How to participate</h2>
            <p> In order to participate, teams will have to register. There is a maximum number of 8 participants in
                each team.
            </p>

            <br></br>


            <p> If you want to participate <em>in any of the first 3 Challenges (VA Estimation, Expr Recognition, or AU
                Detection)</em> you should follow the below procedure for registration.</p>

            <p> The lead researcher should send an email from their official address (no personal emails will be
                accepted) to <a href="mailto: d.kollias@qmul.ac.uk">d.kollias@qmul.ac.uk</a> with: </p>
            <p>i) subject "6th ABAW Competition: Team Registration"; </p>
            <p>ii) <a href="https://drive.google.com/file/d/1StCT5NtE7acGSAYl2ki2gIBDcgwrx6uq/view?usp=sharing">this
                EULA</a> (if the team is composed of only academics) or
                <a href="https://drive.google.com/file/d/153Gwos33iIMjJBlPifrtWIzz7ubzzETA/view?usp=sharing">this
                    EULA</a> (if the team has at least one member coming from the industry) filled in, signed and
                attached; </p>
            <p>iii) the lead researcher's official academic/industrial website; the lead researcher cannot be a student
                (UG/PG/Ph.D.);</p>
            <p>iv) the emails of each team member, each one in a separate line in the body of the email; </p>
            <p>v) the team's name;</p>
            <p>vi) the point of contact name and email address (which member of the team will be the main point of
                contact for future communications, data access etc) </p>


            <p>As a reply, you will receive access to the dataset's cropped/cropped-aligned images and annotations and
                other important information.</p>


            <br></br>


            <p> If you want to participate <em>in the 4th Challenge (CE Recognition)</em> you should follow the below
                procedure for registration.</p>

            <p> The lead researcher should send an email from their official address (no personal emails will be
                accepted) to <a href="mailto: d.kollias@qmul.ac.uk">d.kollias@qmul.ac.uk</a> with: </p>
            <p>i) subject "6th ABAW Competition: Team Registration"; </p>
            <p>ii) <a href="https://drive.google.com/file/d/15wk_3GCjMfuxowGR8d0zSN5w5oHenS41/view?usp=sharing
">this EULA</a> (if the team is composed of only academics) or
                <a href="https://drive.google.com/file/d/1te1rIKVns67NFNSWZs1pINm_-uywgOeX/view?usp=sharing
">this EULA</a> (if the team has at least one member coming from the industry) filled in, signed and attached; </p>
            <p>iii) the lead researcher's official academic/industrial website; the lead researcher cannot be a student
                (UG/PG/Ph.D.);</p>
            <p>iv) the emails of each team member, each one in a separate line in the body of the email; </p>
            <p>v) the team's name;</p>
            <p>vi) the point of contact name and email address (which member of the team will be the main point of
                contact for future communications, data access etc) </p>


            <p>As a reply, you will receive access to the dataset's videos and other important information.</p>


            <br></br>


            <p>If you want to participate in <em> the 5th Challenge </em> please email <a
                    href="mailto: competitions@hume.ai">competitions@hume.ai</a> with the following information:</p>

            <p>i) subject "6th ABAW Competition: Team Registration"</p>
            <p>ii) name and email for the lead researcher's official academic/industrial website; the lead researcher
                cannot be a student (UG/PG/Ph.D.)</p>
            <p>iii) the names and emails of each team member, each one in separate line in the body of the email </p>
            <p>iv) teamâ€™s name</p>
            <p>iv) the point of contact name and email address (which member of the team will be the main point of
                contact for future communications, data access etc) the team's name.</p>

            <p>A reply to sign an EULA will be sent to all team members. When the EULA is signed by all team members a
                link to the data will be shared. </p>


            <br></br>

            <h2>General Information</h2>
            <p> At the end of the Challenges, each team will have to send us:
            </p>

            <p>i) a link to a Github repository where their solution/source code will be stored, </p>
            <p>ii) a link to an ArXiv paper with 2-8 pages describing their proposed methodology, data used and
                results. </p>

            <p>Each team will also need to upload their test set predictions on an evaluation server (details will be
                circulated when the test set is released).</p>

            <p>After that, the winner of each Challenge, along with a leaderboard, will be announced.</p>

            <p>There will be one winner per Challenge. The top-3 performing teams of each Challenge will have to
                contribute paper(s) describing their approach, methodology and results to our Workshop; the accepted
                papers will be part of the CVPR 2024 proceedings. All other teams are also able to submit paper(s)
                describing their solutions and final results; the accepted papers will be part of the CVPR 2024
                proceedings.</p>


            <p>The Competition's white paper (describing the Competition, the data, the baselines and results) will be
                ready at a later stage and will be distributed to the participating teams.</p>


            <br></br>


            <h2>General Rules</h2>

            <p>1) Participants can contribute to any of the 5 Challenges.</p>

            <p>2) In order to take part in any Challenge, participants will have to register as described above.</p>

            <p>3) Any face detector whether commercial or academic can be used in the challenge. The paper accompanying
                the challenge result submission should contain clear details of the detectors/libraries used.</p>

            <p>4) The top performing teams will have to share their solution (code, model weights, executables) with the
                organisers upon completion of the challenge; in this way the organisers will check so as to prevent
                cheating or violation of rules.</p>

            <br></br>

            <div class="section-title">
                <h2>Competition Important Dates</h2>
            </div>


            <br> <span> <em>Call for participation announced, team registration begins, data available: </em>  &nbsp &nbsp &nbsp &nbsp &nbsp  January 13, 2024</span> </br>

            <br> <span> <em>Challenges 1-4 Registration Deadline: </em>  &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp February 18, 2024</span> </br>

            <br> <span> <em>Test set release: </em> &nbsp &nbsp  &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp March 13, 2024</span> </br>

            <br> <span> <em>Final submission deadline (Predictions, Code and ArXiv paper): </em> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  March 19, 2024</span> </br>

            <br> <span> <em>Winners Announcement: </em> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  March 25, 2024</span> </br>

            <br> <span> <em>Final Paper Submission Deadline: </em> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp 23:59:59 AoE (Anywhere on Earth) March 30, 2024  </span> </br>

            <br> <span>  <em>Review decisions sent to authors; Notification of acceptance: </em> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  April 10, 2024 </span> </br>
            <br> <span>   <em>Camera ready version: </em> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp April  14, 2024  </span> </br>


        </div>

    </section><!-- End Clients Section -->


    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->

    <!-- ======= Features Section ======= -->
    <section id="features" class="features">
        <div class="container" data-aos="fade-up">


            <div class="d-flex align-items-center justify-content-center">
                <h2>Valence-Arousal (VA) Estimation Challenge</h2>
            </div>


            <h3>Database</h3>
            For this Challenge, an augmented version of the Aff-Wild2 database will be used. This database is
            audiovisual (A/V), in-the-wild and in total consists of 594 videos of around 3M frames of 584 subjects
            annotated in terms of valence and arousal.


            <br> </br>

            <h3>Rules</h3>
            Only uni-task solutions will be accepted for this Challenge; this means that the teams should only develop
            uni-task (valence-arousal estimation task) solutions.
            Teams are allowed to use any -publicly or not- available pre-trained model (as long as it has not been
            pre-trained on Aff-Wild2). The pre-trained model can be pre-trained on any task (e.g., VA estimation,
            Expression Recognition, AU detection, Face Recognition). However when the teams are refining the model and
            developing the methodology they should not use any other annotations (expressions or AUs): the methodology
            should be purely uni-task, using only the VA annotations. This means that teams are allowed to use other
            databases' VA annotations, or generated/synthetic data, or any affine transformations, or in general data
            augmentation techniques (e.g., <a
                href="https://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Psaroudakis_MixAugment__Mixup_Augmentation_Methods_for_Facial_Expression_Recognition_CVPRW_2022_paper.pdf">MixAugment</a>)
            for increasing the size of the training dataset.


            <br> </br>

            <h3>Performance Assessment</h3>
            <p>
                The performance measure (P) is the mean Concordance Correlation Coefficient (CCC) of valence and
                arousal:
            <table>

                <td>
                    <div class="num"> CCC<sub>arousal</sub> + CCC<sub>valence</sub></div>
                    <div class="denom">2</div>
                </td>
                </tr>
            </table>
            </p>


            <h3>Baseline Results</h3>

            <p>The baseline network is a pre-trained on ImageNet ResNet-50 and its performance on the validation set
                is: </p>
            <p>CCC<sub>valence</sub> = 0.24, &nbsp &nbsp CCC<sub>arousal</sub> = 0.20 </p>
            <p>P = 0.22 </p>


        </div>
    </section><!-- End Features Section -->


    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->

    <!-- ======= Services Section ======= -->
    <section id="services" class="services">
        <div class="container" data-aos="fade-up">


            <div class="d-flex align-items-center justify-content-center">
                <h2> Expression (Expr) Recognition Challenge</h2>
            </div>


            <h3>Database</h3>

            For this Challenge, the Aff-Wild2 database will be used. This database is audiovisual (A/V), in-the-wild and
            in total consists of 548 videos of around 2.7M frames that are annotated in terms of the 6 basic expressions
            (i.e., anger, disgust, fear, happiness, sadness, surprise), plus the neutral state, plus a category 'other'
            that denotes expressions/affective states other than the 6 basic ones.

            <br> </br>

            <h3>Rules</h3>

            Only uni-task solutions will be accepted for this Challenge; this means that the teams should only develop
            uni-task (expression recognition task) solutions.
            Teams are allowed to use any -publicly or not- available pre-trained model (as long as it has not been
            pre-trained on Aff-Wild2). The pre-trained model can be pre-trained on any task (e.g., VA estimation,
            Expression Recognition, AU detection, Face Recognition). However when the teams are refining the model and
            developing the methodology you should not use any other annotations (VA or AUs): the methodology should be
            purely uni-task, using only the Expr annotations. This means that teams are allowed to use other databases'
            Expr annotations, or generated/synthetic data (e.g. the data provided in the ECCV 2022 run of the ABAW
            Challenge), or any affine transformations, or in general data augmentation techniques (e.g., <a
                href="https://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Psaroudakis_MixAugment__Mixup_Augmentation_Methods_for_Facial_Expression_Recognition_CVPRW_2022_paper.pdf">MixAugment</a>)
            for increasing the size of the training dataset.


            <br> </br>

            <h3>Performance Assessment</h3>
            <p>The performance measure (P) is the average F1 Score across all 8 categories: &nbsp
                &sum; F1/8 </p>


            <h3>Baseline Results</h3>
            <p>The baseline network is a pre-trained VGGFACE (with fixed convolutional weights and with <a
                    href="https://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Psaroudakis_MixAugment__Mixup_Augmentation_Methods_for_Facial_Expression_Recognition_CVPRW_2022_paper.pdf">MixAugment</a>
                data augmentation technique) and its performance on the validation set is: </p>
            <p> P = 0.25 </p>


        </div>
    </section><!-- End Services Section -->


    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->


    <!-- ======= Cta Section ======= -->
    <section id="cta" class="cta">
        <div class="container" data-aos="zoom-in">


            <div class="d-flex align-items-center justify-content-center">
                <h2>Action Unit (AU) Detection Challenge</h2>
            </div>


            <h3>Database</h3>

            For this Challenge, the Aff-Wild2 database will be used. This database is audiovisual (A/V), in-the-wild and
            in total consists of 547 videos of around 2.7M frames that are annotated in terms of 12 action units, namely
            AU1,AU2,AU4,AU6,AU7,AU10,AU12,AU15,AU23,AU24,AU25,AU26.


            <br> </br>

            <h3>Rules</h3>

            Only uni-task solutions will be accepted for this Challenge; this means that the teams should only develop
            uni-task (action unit detection task) solutions.
            Teams are allowed to use any -publicly or not- available pre-trained model (as long as it has not been
            pre-trained on Aff-Wild2). The pre-trained model can be pre-trained on any task (e.g., VA estimation,
            Expression Classification, AU detection, Face Recognition). However when the teams are refining the model
            and developing the methodology you should not use any other annotations (VA or Expr): the methodology should
            be purely uni-task, using only the AU annotations. This means that teams are allowed to use other databases'
            AU annotations, or generated/synthetic data, or any affine transformations, or in general data augmentation
            techniques (e.g., <a
                href="https://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Psaroudakis_MixAugment__Mixup_Augmentation_Methods_for_Facial_Expression_Recognition_CVPRW_2022_paper.pdf">MixAugment</a>)
            for increasing the size of the training dataset.


            <br> </br>

            <h3>Performance Assessment</h3>
            <p>The performance measure (P) is the average F1 Score across all 12 categories: &nbsp &sum; F1/12 </p>


            <h3>Baseline Results</h3>
            <p>The baseline network is a pre-trained VGGFACE (with fixed convolutional weights) and its performance on
                the validation set is: </p>
            <p> P = 0.39 </p>


        </div>
    </section><!-- End Cta Section -->


    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->


    <!-- ======= Counts Section ======= -->
    <section id="counts" class="counts">
        <div class="container" data-aos="fade-up">


            <div class="d-flex align-items-center justify-content-center">
                <h2>Compound Expression (CE) Recognition Challenge</h2>
            </div>


            <h3>Database</h3>

            For this Challenge, a part of C-EXPR-DB database will be used (56 videos in total). C-EXPR-DB is audiovisual
            (A/V) in-the-wild database and in total consists of 400 videos of around 200K frames; each frame is
            annotated in terms of 12 compound expressions. For this Challenge, the following 7 compound expressions will
            be considered:
            Fearfully Surprised,
            Happily Surprised,
            Sadly Surprised,
            Disgustedly Surprised,
            Angrily Surprised,
            Sadly Fearful and
            Sadly Angry.
            <br></br>


            <h3>Goal of the Challenge and Rules</h3>
            <p> Participants will be provided with a part of C-EXPR-DB database (56 videos in total), which will be
                unannotated, and will be required to develop their methodologies (supervised/self-supervised, domain
                adaptation, zero-/few-shot learning etc) for recognising the 7 compound expressions in this unannotated
                part, in a per-frame basis. </p>


            <p>Teams are allowed to use any -publicly or not- available pre-trained model and any -publicly or not-
                available database (that contains any annotations, e.g. VA, basic or compound expressions, AUs) </p>


            <h3>Performance Assessment</h3>
            <p>
                The performance measure (P) is the average F1 Score across all 7 categories: &nbsp &sum; F1/7 </p>


        </div>
    </section><!-- End Counts Section -->


    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->


    <!-- ======= Counts Section ======= -->
    <section id="counts2" class="counts2">
        <div class="container" data-aos="fade-up">


            <div class="d-flex align-items-center justify-content-center">
                <h2> Emotional Mimicry Intensity (EMI) Estimation Challenge </h2>
            </div>


            <h3>Database</h3>

            For this Challenge, the multimodal Hume-Vidmimic2 dataset is used which consists of more than 15,000 videos
            totaling over 25 hours. In this dataset, every participant was tasked with imitating a 'seed' video that
            showcased an individual displaying a particular emotion. Following the mimicry, they were then asked to
            assess the emotional intensity of the seed video by selecting from a range of predefined emotional
            categories. The following emotion dimensions are targeted: 'Admiration', 'Amusement', 'Determination',
            'Empathic Pain', 'Excitement', and 'Joy'. A normalized score from 0 to 1 is provided as a ground truth
            value.

            <br></br>


            <h3>Performance Assessment</h3>
            <p>
                The performance measure is the average Pearson's correlation (&rho;) across the 6 emotion dimensions:
                &nbsp &sum; &rho;/6 </p>


            <h3>Baseline Results</h3>
            <p>We established baseline results using two different feature sets.</p>
            <p>First, we employed pre-trained Vision Transformer (ViT) features, which were further processed through a
                three-layer Gated Recurrent Unit (GRU) network. This approach achieved a performance score of:
                0.09. </p>
            <p> Secondly, we utilized features extracted from Wav2Vec2, combined with a linear processing layer, which
                resulted in a performance score of: 0.24. </p>
            <p> Additionally, we explored a multimodal approach by averaging the predictions from both unimodal methods,
                leading to a combined performance score of: 0.25. </p>


        </div>
    </section><!-- End Counts Section -->


    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->


    <!-- ======= Counts Section ======= -->
    <section id="counts3" class="counts3">
        <div class="container" data-aos="fade-up">


            <div class="d-flex align-items-center justify-content-center">
                <h2> Leaderboards </h2>
            </div>

            </br>

            <h3>Valence-Arousal Estimation Challenge:</h3>
            </br>

            In total, 60 Teams participated in the VA Estimation Challenge. 23 Teams submitted their results. 10 Teams
            made invalid (incomplete) submissions, whilst surpassing the baseline. 3 Teams scored lower than the
            baseline.
            10 Teams scored higher than the baseline and made valid submissions.
            </br>
            </br>
            The winner of this Challenge is team Netease Fuxi AI Lab.  </br>
            The runner-up is team DeepAVER.</br>
            In the third place is team CtyunAI. </br>

            </br>
            </br>

            <h3>Expression Recognition Challenge:</h3>
            </br>
            In total, 70 Teams participated in the Expression Recognition Challenge. 40 Teams submitted their results.
            14 Teams made invalid (incomplete) submissions, whilst surpassing the baseline. 16 Teams scored lower than
            the baseline.
            10 Teams scored higher than the baseline and made valid submissions. </br>
            </br>
            The winner of this Challenge is team Netease Fuxi AI Lab.</br>
            The runner-up is team CtyunAI.</br>
            In the third place is team USTC-IAT-United. </br>
            </br>
            </br>


            <h3>Action Unit Detection Challenge:</h3>
            </br>
            In total, 63 Teams participated in the Action Unit Detection Challenge. 40 Teams submitted their results. 16
            Teams made invalid (incomplete) submissions, whilst surpassing the baseline. 17 Teams scored lower than the
            baseline.
            7 Teams scored higher than the baseline and made valid submissions.</br>
            </br>

            The winner of this Challenge is team Netease Fuxi AI Lab.</br>
            The runner-up is team CtyunAI.</br>
            In the third place is team HSEmotion. </br>
            </br>
            </br>


            <h3>Compound Expression Recognition Challenge:</h3>
            </br>
            In total, 40 Teams participated in the Compound Expression Recognition Challenge. 17 Teams submitted their
            results. 12 Teams made invalid (incomplete) submissions.
            5 Teams made valid submissions. </br>
            </br>
            The winner of this Challenge is team Netease Fuxi AI Lab.</br>
            The runner-up is team HSEmotion.</br>
            In the third place is team USTC-IAT-United. </br>
            </br>
            </br>


            <h3>Emotional Mimicry Intensity Estimation Challenge:</h3>
            </br>
            In total, 7 Teams participated in the Emotional Mimicry Intensity Estimation Challenge.
            4 Teams scored higher than the baseline and made valid submissions. </br>
            </br>
            The winner of this Challenge is team Netease Fuxi AI Lab.</br>
            The runner-up is team HCAI-VIS.</br>
            In the third place is team USTC-IAT-United. </br>
            </br>

            </br>
            The leaderboards for all Challenges can be found below:  </br>
            </br>

            <a href="https://drive.google.com/file/d/1Lu1kb4wpgoJL60UI7oQhHhKMOBWg-jQx/view?usp=sharing">CVPR2024_ABAW_Leaderboard
                (first 4 Challenges)</a> </br>

            <a href="https://drive.google.com/file/d/1kQNnCLWprKbl0otQoh9WeheA8YMbFSNX/view?usp=sharing">CVPR2024_ABAW_EMI_Leaderboard
                (EMI Estimation Challenge)</a> </br>


            </br>


            Congratulations to all teams, winning and non-winning ones! Thank you very much for participating in our
            Competition.</br>


            All teams are invited to submit their methodologies-papers (please see Submission Information section
            above). All accepted papers will be part of the IEEE CVPR 2024 proceedings. </br>

            We are looking forward to receiving your submissions! </br>


        </div>
    </section><!-- End Counts Section -->


    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->


    <!-- ======= Cta Section ======= -->
    <section id="testimonials" class="testimonials">
        <div class="container" data-aos="zoom-in">


            <div class="d-flex align-items-center justify-content-center">
                <h2>References</h2>
            </div>

            </br>
            <p>
                If you use the above data, you must cite all following papers (and the white paper that will be
                distributed at a later stage):
            </p>


            <ul>
                <p><i class="ri-check-double-line"></i> D. Kollias, et. al.: "The 6th Affective Behavior Analysis
                    in-the-wild (ABAW) Competition", 2024 </p>
                <p><small>@article{kollias20246th, title={The 6th Affective Behavior Analysis in-the-wild (ABAW)
                    Competition}, author={Kollias, Dimitrios and Tzirakis, Panagiotis and Cowen, Alan and Zafeiriou,
                    Stefanos and Shao, Chunchang and Hu, Guanyu}, journal={arXiv preprint arXiv:2402.19344},
                    year={2024}}</small></p>


                <p><i class="ri-check-double-line"></i> D. Kollias, et. al.: "ABAW: Valence-Arousal Estimation,
                    Expression Recognition, Action Unit Detection & Emotional Reaction Intensity Estimation Challenges".
                    IEEE CVPR, 2023 </p>
                <p><small>@inproceedings{kollias2023abaw2, title={Abaw: Valence-arousal estimation, expression
                    recognition, action unit detection \& emotional reaction intensity estimation challenges},
                    author={Kollias, Dimitrios and Tzirakis, Panagiotis and Baird, Alice and Cowen, Alan and Zafeiriou,
                    Stefanos}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
                    Recognition}, pages={5888--5897}, year={2023}} </small></p>

                <p><i class="ri-check-double-line"></i> D. Kollias: "Multi-Label Compound Expression Recognition: C-EXPR
                    Database & Network". IEEE CVPR, 2023 </p>
                <p><small>@inproceedings{kollias2023multi, title={Multi-Label Compound Expression Recognition: C-EXPR
                    Database \& Network}, author={Kollias, Dimitrios}, booktitle={Proceedings of the IEEE/CVF Conference
                    on Computer Vision and Pattern Recognition}, pages={5589--5598}, year={2023}}</small></p>


                <p><i class="ri-check-double-line"></i> D. Kollias: "ABAW: Learning from Synthetic Data & Multi-Task
                    Learning Challenges". ECCV, 2022 </p>
                <p><small>@inproceedings{kollias2023abaw, title={ABAW: learning from synthetic data \& multi-task
                    learning challenges}, author={Kollias, Dimitrios}, booktitle={European Conference on Computer
                    Vision}, pages={157--172}, year={2023}, organization={Springer} } </small></p>


                <p><i class="ri-check-double-line"></i> D. Kollias: "ABAW: Valence-Arousal Estimation, Expression
                    Recognition, Action Unit Detection & Multi-Task Learning Challenges". IEEE CVPR, 2022 </p>
                <p><small>@inproceedings{kollias2022abaw, title={Abaw: Valence-arousal estimation, expression
                    recognition, action unit detection \& multi-task learning challenges}, author={Kollias, Dimitrios},
                    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
                    pages={2328--2336}, year={2022} }</small></p>


                <p><i class="ri-check-double-line"></i> D. Kollias, et. al.: "Analysing Affective Behavior in the second
                    ABAW2 Competition". ICCV, 2021 </p>
                <p><small>@inproceedings{kollias2021analysing, title={Analysing affective behavior in the second abaw2
                    competition}, author={Kollias, Dimitrios and Zafeiriou, Stefanos}, booktitle={Proceedings of the
                    IEEE/CVF International Conference on Computer Vision}, pages={3652--3660}, year={2021}}</small></p>


                <p><i class="ri-check-double-line"></i> D. Kollias,S. Zafeiriou: "Affect Analysis in-the-wild:
                    Valence-Arousal, Expressions, Action Units and a Unified Framework, 2021 </p>
                <p><small>@article{kollias2021affect, title={Affect Analysis in-the-wild: Valence-Arousal, Expressions,
                    Action Units and a Unified Framework}, author={Kollias, Dimitrios and Zafeiriou, Stefanos},
                    journal={arXiv preprint arXiv:2103.15792}, year={2021}}</small></p>

                <p><i class="ri-check-double-line"></i> D. Kollias, et. al.: "Distribution Matching for Heterogeneous
                    Multi-Task Learning: a Large-scale Face Study", 2021 </p>
                <p><small>@article{kollias2021distribution, title={Distribution Matching for Heterogeneous Multi-Task
                    Learning: a Large-scale Face Study}, author={Kollias, Dimitrios and Sharmanska, Viktoriia and
                    Zafeiriou, Stefanos}, journal={arXiv preprint arXiv:2105.03790}, year={2021} }</small></p>


                <p><i class="ri-check-double-line"></i> D. Kollias, et. al.: "Analysing Affective Behavior in the First
                    ABAW 2020 Competition". IEEE FG, 2020 </p>
                <p><small>@inproceedings{kollias2020analysing, title={Analysing Affective Behavior in the First ABAW
                    2020 Competition}, author={Kollias, D and Schulc, A and Hajiyev, E and Zafeiriou, S},
                    booktitle={2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG
                    2020)(FG)}, pages={794--800}}</small></p>


                <p><i class="ri-check-double-line"></i> D. Kollias, S. Zafeiriou: "Expression, Affect, Action Unit
                    Recognition: Aff-Wild2, Multi-Task Learning and ArcFace". BMVC, 2019 </p>
                <p><small>@article{kollias2019expression, title={Expression, Affect, Action Unit Recognition: Aff-Wild2,
                    Multi-Task Learning and ArcFace}, author={Kollias, Dimitrios and Zafeiriou, Stefanos},
                    journal={arXiv preprint arXiv:1910.04855}, year={2019}}</small></p>


                <p><i class="ri-check-double-line"></i> D. Kollias, et. al.: "Deep Affect Prediction in-the-wild:
                    Aff-Wild Database and Challenge, Deep Architectures, and Beyond". International Journal of Computer
                    Vision (IJCV), 2019 </p>
                <p><small>@article{kollias2019deep, title={Deep affect prediction in-the-wild: Aff-wild database and
                    challenge, deep architectures, and beyond}, author={Kollias, Dimitrios and Tzirakis, Panagiotis and
                    Nicolaou, Mihalis A and Papaioannou, Athanasios and Zhao, Guoying and Schuller, Bj{\"o}rn and
                    Kotsia, Irene and Zafeiriou, Stefanos}, journal={International Journal of Computer Vision},
                    pages={1--23}, year={2019}, publisher={Springer} }</small></p>


                <p><i class="ri-check-double-line"></i> D. D. Kollias, et at.: "Face Behavior a la carte: Expressions,
                    Affect and Action Units in a Single Network", 2019 </p>
                <p><small>@article{kollias2019face,title={Face Behavior a la carte: Expressions, Affect and Action Units
                    in a Single Network}, author={Kollias, Dimitrios and Sharmanska, Viktoriia and Zafeiriou, Stefanos},
                    journal={arXiv preprint arXiv:1910.11111}, year={2019}}</small></p>


                <p><i class="ri-check-double-line"></i> S. Zafeiriou, et. al. "Aff-Wild: Valence and Arousal in-the-wild
                    Challenge". IEEE CVPR, 2017</p>
                <p><small>@inproceedings{zafeiriou2017aff, title={Aff-wild: Valence and arousal â€˜in-the-wildâ€™challenge},
                    author={Zafeiriou, Stefanos and Kollias, Dimitrios and Nicolaou, Mihalis A and Papaioannou,
                    Athanasios and Zhao, Guoying and Kotsia, Irene}, booktitle={Computer Vision and Pattern Recognition
                    Workshops (CVPRW), 2017 IEEE Conference on}, pages={1980--1987}, year={2017}, organization={IEEE}
                    } </small></p>


            </ul>


        </div>
    </section><!-- End Counts Section -->


    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->
    <!-- ================================================================================================== -->


    <!-- ======= Cta Section ======= -->
    <section id="testimonials" class="testimonials">
        <div class="container" data-aos="zoom-in">


            <div class="d-flex align-items-center justify-content-center">
                <h2>Sponsors</h2>
            </div>

            </br>
            <p>
                The Affective Behavior Analysis in-the-wild Workshop and Competition has been generously supported by:

            </p>

            <ul>
                <p><i class="ri-check-double-line"></i> Queen Mary University of London </p>
                <p><small> <img src="img/qmul.jpeg" alt="QMUL"> </small></p>

                <p><i class="ri-check-double-line"></i> Imperial College London </p>
                <p><small> <img src="img/icl.png" alt="ICL"> </small></p>


                <p><i class="ri-check-double-line"></i> Hume AI </p>
                <p><small> <img src="img/hume.jpeg" alt="HUME"> </small></p>


            </ul>


        </div>
    </section><!-- End Counts Section -->


    <!-- Comments go here
    ftiaxe faq


              <div class="col-lg-4 col-md-6 d-flex align-items-stretch mt-4" data-aos="zoom-in" data-aos-delay="300">
                <div class="icon-box">
                  <div class="icon"><i class="bx bx-arch"></i></div>
                  <h4><a href="">Divera don</a></h4>
                  <p>Modi nostrum vel laborum. Porro fugit error sit minus sapiente sit aspernatur</p>
                </div>
              </div>

            </div>

          </div>
        </section>








        <section id="testimonials" class="testimonials">
          <div class="container" data-aos="zoom-in">

            <div class="testimonials-slider swiper" data-aos="fade-up" data-aos-delay="100">
              <div class="swiper-wrapper">

                <div class="swiper-slide">
                  <div class="testimonial-item">
                    <img src="img/testimonials/testimonials-1.jpg" class="testimonial-img" alt="">
                    <h3>Saul Goodman</h3>
                    <h4>Ceo &amp; Founder</h4>
                    <p>
                      <i class="bx bxs-quote-alt-left quote-icon-left"></i>
                      Proin iaculis purus consequat sem cure digni ssim donec porttitora entum suscipit rhoncus. Accusantium quam, ultricies eget id, aliquam eget nibh et. Maecen aliquam, risus at semper.
                      <i class="bx bxs-quote-alt-right quote-icon-right"></i>
                    </p>
                  </div>
                </div>

                <div class="swiper-slide">
                  <div class="testimonial-item">
                    <img src="img/testimonials/testimonials-2.jpg" class="testimonial-img" alt="">
                    <h3>Sara Wilsson</h3>
                    <h4>Designer</h4>
                    <p>
                      <i class="bx bxs-quote-alt-left quote-icon-left"></i>
                      Export tempor illum tamen malis malis eram quae irure esse labore quem cillum quid cillum eram malis quorum velit fore eram velit sunt aliqua noster fugiat irure amet legam anim culpa.
                      <i class="bx bxs-quote-alt-right quote-icon-right"></i>
                    </p>
                  </div>
                </div>

                <div class="swiper-slide">
                  <div class="testimonial-item">
                    <img src="img/testimonials/testimonials-3.jpg" class="testimonial-img" alt="">
                    <h3>Jena Karlis</h3>
                    <h4>Store Owner</h4>
                    <p>
                      <i class="bx bxs-quote-alt-left quote-icon-left"></i>
                      Enim nisi quem export duis labore cillum quae magna enim sint quorum nulla quem veniam duis minim tempor labore quem eram duis noster aute amet eram fore quis sint minim.
                      <i class="bx bxs-quote-alt-right quote-icon-right"></i>
                    </p>
                  </div>
                </div>

                <div class="swiper-slide">
                  <div class="testimonial-item">
                    <img src="img/testimonials/testimonials-4.jpg" class="testimonial-img" alt="">
                    <h3>Matt Brandon</h3>
                    <h4>Freelancer</h4>
                    <p>
                      <i class="bx bxs-quote-alt-left quote-icon-left"></i>
                      Fugiat enim eram quae cillum dolore dolor amet nulla culpa multos export minim fugiat minim velit minim dolor enim duis veniam ipsum anim magna sunt elit fore quem dolore labore illum veniam.
                      <i class="bx bxs-quote-alt-right quote-icon-right"></i>
                    </p>
                  </div>
                </div>

                <div class="swiper-slide">
                  <div class="testimonial-item">
                    <img src="img/testimonials/testimonials-5.jpg" class="testimonial-img" alt="">
                    <h3>John Larson</h3>
                    <h4>Entrepreneur</h4>
                    <p>
                      <i class="bx bxs-quote-alt-left quote-icon-left"></i>
                      Quis quorum aliqua sint quem legam fore sunt eram irure aliqua veniam tempor noster veniam enim culpa labore duis sunt culpa nulla illum cillum fugiat legam esse veniam culpa fore nisi cillum quid.
                      <i class="bx bxs-quote-alt-right quote-icon-right"></i>
                    </p>
                  </div>
                </div>
              </div>
              <div class="swiper-pagination"></div>
            </div>

          </div>
        </section>



        <section id="contact" class="contact">
          <div class="container" data-aos="fade-up">

            <div class="section-title">
              <h2>Contact</h2>
              <p>Contact Us</p>
            </div>

            <div>
              <iframe style="border:0; width: 100%; height: 270px;" src="https://www.google.com/maps/embed?pb=!1m14!1m8!1m3!1d12097.433213460943!2d-74.0062269!3d40.7101282!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x0%3A0xb89d1fe6bc499443!2sDowntown+Conference+Center!5e0!3m2!1smk!2sbg!4v1539943755621" frameborder="0" allowfullscreen></iframe>
            </div>

            <div class="row mt-5">

              <div class="col-lg-4">
                <div class="info">
                  <div class="address">
                    <i class="bi bi-geo-alt"></i>
                    <h4>Location:</h4>
                    <p>A108 Adam Street, New York, NY 535022</p>
                  </div>

                  <div class="email">
                    <i class="bi bi-envelope"></i>
                    <h4>Email:</h4>
                    <p>info@example.com</p>
                  </div>

                  <div class="phone">
                    <i class="bi bi-phone"></i>
                    <h4>Call:</h4>
                    <p>+1 5589 55488 55s</p>
                  </div>

                </div>

              </div>

              <div class="col-lg-8 mt-5 mt-lg-0">

                <form action="forms/contact.php" method="post" role="form" class="php-email-form">
                  <div class="row">
                    <div class="col-md-6 form-group">
                      <input type="text" name="name" class="form-control" id="name" placeholder="Your Name" required>
                    </div>
                    <div class="col-md-6 form-group mt-3 mt-md-0">
                      <input type="email" class="form-control" name="email" id="email" placeholder="Your Email" required>
                    </div>
                  </div>
                  <div class="form-group mt-3">
                    <input type="text" class="form-control" name="subject" id="subject" placeholder="Subject" required>
                  </div>
                  <div class="form-group mt-3">
                    <textarea class="form-control" name="message" rows="5" placeholder="Message" required></textarea>
                  </div>
                  <div class="my-3">
                    <div class="loading">Loading</div>
                    <div class="error-message"></div>
                    <div class="sent-message">Your message has been sent. Thank you!</div>
                  </div>
                  <div class="text-center"><button type="submit">Send Message</button></div>
                </form>

              </div>

            </div>

          </div>
        </section>
    -->


</main><!-- End #main -->


<footer id="footer">
    <div class="footer-top">
        <div class="container">
            <div class="row">

                <div class="container">
                    <div class="copyright">
                        &copy; Copyright <strong><span>Gp & Dimitrios Kollias</span></strong>. All Rights Reserved
                    </div>
                    <div class="credits">
                        Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
                    </div>
                </div>
</footer>
<div id="preloader"></div>
<a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i
        class="bi bi-arrow-up-short"></i></a>

</div>
</div>
</div>


<!-- Vendor JS Files -->
<script src="vendor/purecounter/purecounter_vanilla.js"></script>
<script src="vendor/aos/aos.js"></script>
<script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
<script src="vendor/glightbox/js/glightbox.min.js"></script>
<script src="vendor/isotope-layout/isotope.pkgd.min.js"></script>
<script src="vendor/swiper/swiper-bundle.min.js"></script>
<script src="vendor/php-email-form/validate.js"></script>

<!-- Template Main JS File -->
<script src="js/main.js"></script>

</body>

</html>
